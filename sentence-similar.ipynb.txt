{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%cd /kaggle/input/similartext/Measuring-Sentence-Similarity-master","metadata":{"execution":{"iopub.status.busy":"2023-10-20T01:57:33.281774Z","iopub.execute_input":"2023-10-20T01:57:33.282694Z","iopub.status.idle":"2023-10-20T01:57:33.333396Z","shell.execute_reply.started":"2023-10-20T01:57:33.282645Z","shell.execute_reply":"2023-10-20T01:57:33.332205Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/similartext/Measuring-Sentence-Similarity-master\n","output_type":"stream"}]},{"cell_type":"code","source":"%cp -R \"/usr/share/nltk_data\" nltk/","metadata":{"execution":{"iopub.status.busy":"2023-10-20T02:13:54.772357Z","iopub.execute_input":"2023-10-20T02:13:54.772775Z","iopub.status.idle":"2023-10-20T02:13:55.904275Z","shell.execute_reply.started":"2023-10-20T02:13:54.772743Z","shell.execute_reply":"2023-10-20T02:13:55.902790Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"cp: cannot create directory 'nltk/': Read-only file system\n","output_type":"stream"}]},{"cell_type":"code","source":"!sudo chmod -R 777 \"/usr/share/nltk_data\"","metadata":{"execution":{"iopub.status.busy":"2023-10-20T02:13:24.429185Z","iopub.execute_input":"2023-10-20T02:13:24.429618Z","iopub.status.idle":"2023-10-20T02:13:25.687397Z","shell.execute_reply.started":"2023-10-20T02:13:24.429585Z","shell.execute_reply":"2023-10-20T02:13:25.685869Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"!sudo zip /kaggle/working/nltk_data.zip /usr/share/nltk_data","metadata":{"execution":{"iopub.status.busy":"2023-10-20T02:16:13.266083Z","iopub.execute_input":"2023-10-20T02:16:13.266584Z","iopub.status.idle":"2023-10-20T02:16:14.406147Z","shell.execute_reply.started":"2023-10-20T02:16:13.266489Z","shell.execute_reply":"2023-10-20T02:16:14.404661Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"  adding: usr/share/nltk_data/ (stored 0%)\n","output_type":"stream"}]},{"cell_type":"code","source":"!python3 -m nltk.downloader wordnet\n!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/","metadata":{"execution":{"iopub.status.busy":"2023-10-20T01:57:33.335658Z","iopub.execute_input":"2023-10-20T01:57:33.336466Z","iopub.status.idle":"2023-10-20T01:57:39.132827Z","shell.execute_reply.started":"2023-10-20T01:57:33.336426Z","shell.execute_reply":"2023-10-20T01:57:39.131055Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/opt/conda/lib/python3.10/runpy.py:126: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n  warn(RuntimeWarning(msg))\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\nArchive:  /usr/share/nltk_data/corpora/wordnet.zip\n   creating: /usr/share/nltk_data/corpora/wordnet/\n  inflating: /usr/share/nltk_data/corpora/wordnet/lexnames  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.verb  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.adv  \n  inflating: /usr/share/nltk_data/corpora/wordnet/adv.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.verb  \n  inflating: /usr/share/nltk_data/corpora/wordnet/cntlist.rev  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.adj  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.adj  \n  inflating: /usr/share/nltk_data/corpora/wordnet/LICENSE  \n  inflating: /usr/share/nltk_data/corpora/wordnet/citation.bib  \n  inflating: /usr/share/nltk_data/corpora/wordnet/noun.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/verb.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/README  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.sense  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.noun  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.adv  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.noun  \n  inflating: /usr/share/nltk_data/corpora/wordnet/adj.exc  \n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\n\ndataset = load_dataset(\"mteb/sickr-sts\")","metadata":{"execution":{"iopub.status.busy":"2023-10-20T01:57:39.135107Z","iopub.execute_input":"2023-10-20T01:57:39.135901Z","iopub.status.idle":"2023-10-20T01:57:42.229124Z","shell.execute_reply.started":"2023-10-20T01:57:39.135865Z","shell.execute_reply":"2023-10-20T01:57:42.228042Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Downloading and preparing dataset json/mteb--sickr-sts to /root/.cache/huggingface/datasets/json/mteb--sickr-sts-1e81327897d49df9/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9f4b81e269545be9b92ad3c535c53dc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/175k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a376a5fd9c742f595a92cd041e0f494"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0b9c025b12442df9e441a5b678861ae"}},"metadata":{}},{"name":"stdout","text":"Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/mteb--sickr-sts-1e81327897d49df9/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f339c32495e444f19ea7eae90970eabb"}},"metadata":{}}]},{"cell_type":"code","source":"dataset['test']","metadata":{"execution":{"iopub.status.busy":"2023-10-20T01:57:42.231594Z","iopub.execute_input":"2023-10-20T01:57:42.232274Z","iopub.status.idle":"2023-10-20T01:57:42.240810Z","shell.execute_reply.started":"2023-10-20T01:57:42.232217Z","shell.execute_reply":"2023-10-20T01:57:42.239326Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['sentence1', 'sentence2', 'score'],\n    num_rows: 9927\n})"},"metadata":{}}]},{"cell_type":"code","source":"import sys\nimport pandas as pd\nimport numpy as np\nfrom pre_processor import Preprocess\nfrom semantic_features import SemanticFeatures\nfrom syntactic_features import SyntacticFeatures\nfrom machine_learning_processor import MachineLearningClassifier\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.metrics import mean_absolute_error,mean_squared_error\n\n\"\"\"\n\nCODE SHEET :\nONE = COSINE_SIMILARITY\nTWO = JACCARD_SIMILARITY\nTHREE = LEMMA_JACCARD_SIMILARITY\nFOUR = COMBINED_SYNTACTIC\n\nFIVE = PATH_SIMILARITY\nSIX = WUP_SIMILARITY\nSEVEN = COMBINED_SEMANTIC\n\nEIGHT = SVM\nNINE = LOGISTIC_REGRESSION\n\"\"\"\nfrom sklearn.linear_model import LinearRegression\n\nclass MeasureSimilarity(object):\n    def __init__(self, feature_code, classifier_code):\n        print(\"hello\")\n        self.training_dataset_size = 3000\n        self.testing_dataset_size = 1000\n        self.feature_code = feature_code\n        self.classifier_code = classifier_code\n        print(\" feature_code : \",self.feature_code)\n        print(\" classifier_code : \",self.classifier_code)\n        self.data_setup()\n\n    def data_setup(self):\n        \"\"\" sets up data and call functions for feature generations and classifer\"\"\"\n        question_list1 = [dataset['test'][i]['sentence1'] for i in range(len(dataset['test']))]\n        question_list2 = [dataset['test'][i]['sentence2'] for i in range(len(dataset['test']))]\n        is_duplicate = [dataset['test'][i]['score'] for i in range(len(dataset['test']))]\n        \n        # for will\n        X = []\n        Y = []\n        print(\"Generate input ...\")\n        for i in range(0, 9000):\n            if (i%500==0):\n                print(\"*\"*20, i ,\"*\"*20 )\n            feature1 = self.call_feature_generator(question_list1[i],question_list2[i], 'THREE')\n            feature2 = self.call_feature_generator(question_list1[i],question_list2[i], 'FIVE')\n            X.append([feature1, feature2])\n            Y.append(is_duplicate[i])\n\n        # we train classifier\n        reg = LinearRegression().fit(X, Y)\n\n        #  testing\n        testX = []\n        testY = []\n\n        for i in range(9000, 9927):\n            feature1 = self.call_feature_generator(question_list1[i],question_list2[i], 'THREE')\n            feature2 = self.call_feature_generator(question_list1[i],question_list2[i], 'FIVE')\n            testX.append([feature1, feature2])\n            testY.append(is_duplicate[i])\n\n        print('Results:')\n        \n        y_pred = reg.predict(testX)\n        mae = mean_absolute_error(y_true=testY,y_pred=y_pred)\n        mse = mean_squared_error(y_true=testY,y_pred=y_pred)\n        rmse = mean_squared_error(y_true=testY,y_pred=y_pred,squared=False)\n \n        print(\"MAE:\",mae)\n        print(\"MSE:\",mse)\n        print(\"RMSE:\",rmse)\n\n       \n\n    def call_classifier(self, x, y, code):\n        \"\"\" control the implementation \"\"\"\n        if code == \"EIGHT\":\n            classifier = MachineLearningClassifier.svm_classifier(x, y)\n            return classifier\n        elif code == \"NINE\":\n            classifier = MachineLearningClassifier.logistic_regression_classifier(x, y)\n            return classifier\n        else:\n            raise ValueError('Enter correct Code for classifier')\n\n\n    def call_feature_generator(self, ques1, ques2, code):\n        # using lemma\n        processer1 = Preprocess(ques1)\n        lemma_ques1 = processer1.preprocess_with_lemma()\n\n        processer2 = Preprocess(ques2)\n        lemma_ques2 = processer2.preprocess_with_lemma()\n\n        # without using lemma\n        processer1 = Preprocess(ques1)\n        token_ques1 = processer1.preprocess_without_lemma()\n\n        processer2 = Preprocess(ques2)\n        token_ques2 = processer2.preprocess_without_lemma()\n\n\n        syntactic_obj = SyntacticFeatures()\n        semantic_obj = SemanticFeatures()\n\n        if code == \"ONE\":\n            cosine_similarity_score = syntactic_obj.compute_cosine_similarity(token_ques1, token_ques2)\n            return cosine_similarity_score\n        elif code == \"TWO\":\n            jaccard_similarity_score = syntactic_obj.compute_jaccard_similarity(token_ques1, token_ques2)\n            return jaccard_similarity_score\n        elif code == \"THREE\":\n            lemma_jaccard_similarity_score = syntactic_obj.compute_lemma_jaccard_similarity(lemma_ques1, lemma_ques2)\n            return lemma_jaccard_similarity_score\n        elif code == \"FOUR\":\n            combined_syn_score = syntactic_obj.overall_similarity_combined(token_ques1, token_ques2, lemma_ques1, lemma_ques2)\n            return combined_syn_score\n        elif code == \"FIVE\":\n            path_similarity_score = semantic_obj.overall_similarity_path_similarity(lemma_ques1, lemma_ques2)\n            return path_similarity_score\n        elif code == \"SIX\":\n            wup_similarity_score = semantic_obj.overall_similarity_wup_similarity(lemma_ques1, lemma_ques2)\n            return wup_similarity_score\n        elif code == \"SEVEN\":\n            combined_similarity_score = semantic_obj.overall_similarity_combined(lemma_ques1, lemma_ques2)\n            return combined_similarity_score\n        else:\n            raise ValueError('Enter correct Code for feature')","metadata":{"execution":{"iopub.status.busy":"2023-10-20T01:57:42.242428Z","iopub.execute_input":"2023-10-20T01:57:42.242746Z","iopub.status.idle":"2023-10-20T01:57:43.287715Z","shell.execute_reply.started":"2023-10-20T01:57:42.242718Z","shell.execute_reply":"2023-10-20T01:57:43.286161Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"my_obj = MeasureSimilarity('FIVE', \"EIGHT\")","metadata":{"execution":{"iopub.status.busy":"2023-10-20T01:57:43.289580Z","iopub.execute_input":"2023-10-20T01:57:43.290603Z","iopub.status.idle":"2023-10-20T01:58:02.277431Z","shell.execute_reply.started":"2023-10-20T01:57:43.290562Z","shell.execute_reply":"2023-10-20T01:58:02.275389Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"hello\n feature_code :  FIVE\n classifier_code :  EIGHT\nGenerate input ...\n******************** 0 ********************\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m my_obj \u001b[38;5;241m=\u001b[39m \u001b[43mMeasureSimilarity\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mFIVE\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEIGHT\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[5], line 37\u001b[0m, in \u001b[0;36mMeasureSimilarity.__init__\u001b[0;34m(self, feature_code, classifier_code)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m feature_code : \u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_code)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m classifier_code : \u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier_code)\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_setup\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[5], line 53\u001b[0m, in \u001b[0;36mMeasureSimilarity.data_setup\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m20\u001b[39m, i ,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m20\u001b[39m )\n\u001b[1;32m     52\u001b[0m feature1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_feature_generator(question_list1[i],question_list2[i], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTHREE\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 53\u001b[0m feature2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_feature_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion_list1\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mquestion_list2\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mFIVE\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m X\u001b[38;5;241m.\u001b[39mappend([feature1, feature2])\n\u001b[1;32m     55\u001b[0m Y\u001b[38;5;241m.\u001b[39mappend(is_duplicate[i])\n","Cell \u001b[0;32mIn[5], line 127\u001b[0m, in \u001b[0;36mMeasureSimilarity.call_feature_generator\u001b[0;34m(self, ques1, ques2, code)\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m combined_syn_score\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m code \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFIVE\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 127\u001b[0m     path_similarity_score \u001b[38;5;241m=\u001b[39m \u001b[43msemantic_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moverall_similarity_path_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlemma_ques1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlemma_ques2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m path_similarity_score\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m code \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSIX\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n","File \u001b[0;32m/kaggle/input/similartext/Measuring-Sentence-Similarity-master/semantic_features.py:92\u001b[0m, in \u001b[0;36mSemanticFeatures.overall_similarity_path_similarity\u001b[0;34m(ques1, ques2)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moverall_similarity_path_similarity\u001b[39m(ques1, ques2):\n\u001b[1;32m     91\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" calculate path similarity \"\"\"\u001b[39;00m\n\u001b[0;32m---> 92\u001b[0m     sentence_means1 \u001b[38;5;241m=\u001b[39m \u001b[43mSemanticFeatures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_lesk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mques1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m     sentence_means2 \u001b[38;5;241m=\u001b[39m SemanticFeatures\u001b[38;5;241m.\u001b[39mget_lesk(ques2)\n\u001b[1;32m     95\u001b[0m     R1 \u001b[38;5;241m=\u001b[39m SemanticFeatures\u001b[38;5;241m.\u001b[39mcompute_path_similarity(sentence_means1, sentence_means2)\n","File \u001b[0;32m/kaggle/input/similartext/Measuring-Sentence-Similarity-master/semantic_features.py:19\u001b[0m, in \u001b[0;36mSemanticFeatures.get_lesk\u001b[0;34m(ques)\u001b[0m\n\u001b[1;32m     17\u001b[0m sentence_means \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m ques:\n\u001b[0;32m---> 19\u001b[0m     sentence_means\u001b[38;5;241m.\u001b[39mappend(\u001b[43mlesk_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlesk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mques\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sentence_means\n","File \u001b[0;32m/kaggle/input/similartext/Measuring-Sentence-Similarity-master/utils/lesk_algorithm.py:92\u001b[0m, in \u001b[0;36mLesk.lesk\u001b[0;34m(self, word, sentence)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word_context \u001b[38;5;129;01min\u001b[39;00m context:\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m word \u001b[38;5;241m==\u001b[39m word_context:\n\u001b[0;32m---> 92\u001b[0m         score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moverlapScore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword_context\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m score[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     94\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n","File \u001b[0;32m/kaggle/input/similartext/Measuring-Sentence-Similarity-master/utils/lesk_algorithm.py:57\u001b[0m, in \u001b[0;36mLesk.overlapScore\u001b[0;34m(self, word1, word2)\u001b[0m\n\u001b[1;32m     55\u001b[0m gloss_set1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgetAll(word1)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeanings[word2] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 57\u001b[0m     gloss_set2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetAll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# print 'here'\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     gloss_set2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgetGloss([wn\u001b[38;5;241m.\u001b[39msynset(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeanings[word2])])\n","File \u001b[0;32m/kaggle/input/similartext/Measuring-Sentence-Similarity-master/utils/lesk_algorithm.py:40\u001b[0m, in \u001b[0;36mLesk.getAll\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m senses \u001b[38;5;241m==\u001b[39m []:\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {word\u001b[38;5;241m.\u001b[39mlower(): senses}\n\u001b[0;32m---> 40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetGloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43msenses\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/kaggle/input/similartext/Measuring-Sentence-Similarity-master/utils/lesk_algorithm.py:30\u001b[0m, in \u001b[0;36mLesk.getGloss\u001b[0;34m(self, senses)\u001b[0m\n\u001b[1;32m     27\u001b[0m     gloss[sense\u001b[38;5;241m.\u001b[39mname()] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sense \u001b[38;5;129;01min\u001b[39;00m senses:\n\u001b[0;32m---> 30\u001b[0m     gloss[sense\u001b[38;5;241m.\u001b[39mname()] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msense\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefinition\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m gloss\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/tokenize/__init__.py:130\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    116\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    :type preserver_line: bool\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 130\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences\n\u001b[1;32m    132\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/tokenize/__init__.py:97\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;124;03mReturn a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;124;03musing NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;124;03m:param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     96\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m load(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokenizers/punkt/\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m.pickle\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(language))\n\u001b[0;32m---> 97\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/tokenize/punkt.py:1235\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.tokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1231\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize\u001b[39m(\u001b[38;5;28mself\u001b[39m, text, realign_boundaries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m   1232\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1233\u001b[0m \u001b[38;5;124;03m    Given a text, returns a list of the sentences in that text.\u001b[39;00m\n\u001b[1;32m   1234\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1235\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msentences_from_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrealign_boundaries\u001b[49m\u001b[43m)\u001b[49m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/tokenize/punkt.py:1283\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.sentences_from_text\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1276\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msentences_from_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, text, realign_boundaries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m   1277\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1278\u001b[0m \u001b[38;5;124;03m    Given a text, generates the sentences in that text by only\u001b[39;00m\n\u001b[1;32m   1279\u001b[0m \u001b[38;5;124;03m    testing candidate sentence breaks. If realign_boundaries is\u001b[39;00m\n\u001b[1;32m   1280\u001b[0m \u001b[38;5;124;03m    True, includes in the sentence closing punctuation that\u001b[39;00m\n\u001b[1;32m   1281\u001b[0m \u001b[38;5;124;03m    follows the period.\u001b[39;00m\n\u001b[1;32m   1282\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [text[s:e] \u001b[38;5;28;01mfor\u001b[39;00m s, e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspan_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrealign_boundaries\u001b[49m\u001b[43m)\u001b[49m]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/tokenize/punkt.py:1274\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.span_tokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1272\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m realign_boundaries:\n\u001b[1;32m   1273\u001b[0m     slices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_realign_boundaries(text, slices)\n\u001b[0;32m-> 1274\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [(sl\u001b[38;5;241m.\u001b[39mstart, sl\u001b[38;5;241m.\u001b[39mstop) \u001b[38;5;28;01mfor\u001b[39;00m sl \u001b[38;5;129;01min\u001b[39;00m slices]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/tokenize/punkt.py:1274\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1272\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m realign_boundaries:\n\u001b[1;32m   1273\u001b[0m     slices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_realign_boundaries(text, slices)\n\u001b[0;32m-> 1274\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [(sl\u001b[38;5;241m.\u001b[39mstart, sl\u001b[38;5;241m.\u001b[39mstop) \u001b[38;5;28;01mfor\u001b[39;00m sl \u001b[38;5;129;01min\u001b[39;00m slices]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/tokenize/punkt.py:1314\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._realign_boundaries\u001b[0;34m(self, text, slices)\u001b[0m\n\u001b[1;32m   1301\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1302\u001b[0m \u001b[38;5;124;03mAttempts to realign punctuation that falls after the period but\u001b[39;00m\n\u001b[1;32m   1303\u001b[0m \u001b[38;5;124;03mshould otherwise be included in the same sentence.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1311\u001b[0m \u001b[38;5;124;03m    [\"(Sent1.)\", \"Sent2.\"].\u001b[39;00m\n\u001b[1;32m   1312\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1313\u001b[0m realign \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1314\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sl1, sl2 \u001b[38;5;129;01min\u001b[39;00m _pair_iter(slices):\n\u001b[1;32m   1315\u001b[0m     sl1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mslice\u001b[39m(sl1\u001b[38;5;241m.\u001b[39mstart \u001b[38;5;241m+\u001b[39m realign, sl1\u001b[38;5;241m.\u001b[39mstop)\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sl2:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/tokenize/punkt.py:312\u001b[0m, in \u001b[0;36m_pair_iter\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;124;03mYields pairs of tokens from the given iterator such that each input\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;124;03mtoken will appear as the first element in a yielded tuple. The last\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;124;03mpair will have None as its second element.\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    311\u001b[0m it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(it)\n\u001b[0;32m--> 312\u001b[0m prev \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m el \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m (prev, el)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/tokenize/punkt.py:1298\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._slices_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1296\u001b[0m             last_break \u001b[38;5;241m=\u001b[39m match\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m   1297\u001b[0m \u001b[38;5;66;03m# The last sentence should not contain trailing whitespace.\u001b[39;00m\n\u001b[0;32m-> 1298\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mslice\u001b[39m(last_break, \u001b[38;5;28mlen\u001b[39m(\u001b[43mtext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrstrip\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m))\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}